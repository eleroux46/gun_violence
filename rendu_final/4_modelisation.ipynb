{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_violence_db = pd.read_csv('data/gun_violence_db.csv')\n",
    "merge_geo=pd.read_csv('data/merge_geo.csv')\n",
    "counties_db = pd.read_csv('data/counties_db.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous proposons une tentative de modélisation de la fréquence d'incidents, du nombre de tués et du nombre de blessés au total par an, par comté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: resident_pop_year_2015'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mgrenier-164589-0.user.lab.sspcloud.fr/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m state_incident_counts \u001b[39m=\u001b[39m merge_geo\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mincident_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mgrenier-164589-0.user.lab.sspcloud.fr/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m state_incident_counts\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum_incident\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-mgrenier-164589-0.user.lab.sspcloud.fr/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m state_population \u001b[39m=\u001b[39m counties_db\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mEtat\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39mresident_pop_year_2015\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mgrenier-164589-0.user.lab.sspcloud.fr/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m state_population\u001b[39m.\u001b[39mcolumns\u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpop\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mgrenier-164589-0.user.lab.sspcloud.fr/home/onyxia/gun_violence/rendu_final/4_modelisation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m state_incident_counts[\u001b[39m'\u001b[39m\u001b[39mincident_per_1K\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m state_incident_counts[\u001b[39m'\u001b[39m\u001b[39msum_incident\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39mstate_population[\u001b[39m'\u001b[39m\u001b[39mpop\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(key) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1958\u001b[0m     \u001b[39m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     \u001b[39m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a list instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[0;32m-> 1964\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn not found: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj[key]\u001b[39m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gotitem(key, ndim\u001b[39m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: resident_pop_year_2015'"
     ]
    }
   ],
   "source": [
    "state_incident_counts = merge_geo.groupby('state')['incident_id'].count().reset_index()\n",
    "state_incident_counts.columns = ['state', 'sum_incident']\n",
    "\n",
    "state_population = counties_db.groupby('Etat')['resident_pop_year_2015'].sum().reset_index()\n",
    "state_population.columns= ['state', 'pop']\n",
    "\n",
    "state_incident_counts['incident_per_1K'] = state_incident_counts['sum_incident']/state_population['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_state = total_db.groupby('Etat')['score_legis'].mean().reset_index()\n",
    "score_by_state.columns = ['state', 'score_legis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(left=state_incident_counts, right = score_by_state, on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.sort_values(by='incident_per_1K').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_data['score_legis'], merged_data['incident_per_1K'])\n",
    "plt.title('Legislation Score vs. Incidents per Capita by State')\n",
    "plt.xlabel('Legislation Score')\n",
    "plt.ylabel('Incident Rate per Capita')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_state = total_db.groupby('Etat').mean('per_dem').reset_index()[['Etat', 'per_dem']]\n",
    "dem_state.columns = ['state', 'per_dem']\n",
    "merged_data = pd.merge(left=merged_data, right=dem_state, on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['per_dem'].corr(merged_data['incident_per_1K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "merged_data = merged_data.dropna(subset=['incident_per_1K', 'score_legis'])\n",
    "# Assuming heat_data is your DataFrame and 'column1' and 'column2' are the column names\n",
    "result = linregress(merged_data[['incident_per_1K', 'score_legis']])\n",
    "\n",
    "# The result object contains various statistics, including the slope and intercept\n",
    "slope = result.slope\n",
    "intercept = result.intercept\n",
    "correlation_coefficient = result.rvalue\n",
    "p_value = result.pvalue\n",
    "standard_error = result.stderr\n",
    "\n",
    "# Print the results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Standard Error: {standard_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rvalue**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, tentative de heatmap avec variables intéressantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_data = total_db[['fips', 'resident_pop_year_2015','med_h_income_year_2015', 'unemp_rate_year_2015', 'snap_beneficiaries_year_2015', 'bachelors_deg_year_2015', 'bchecks_2015', 'score_legis', 'per_dem']]\n",
    "counties_data['pop_density'] = total_db['resident_pop_year_2015']/total_db['geometry'].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_data['area'] = total_db_geo['geometry'].area\n",
    "states_data = counties_data.groupby('fips').agg({\n",
    "    'med_h_income_year_2015': 'mean',\n",
    "    'unemp_rate_year_2015': 'mean',\n",
    "    'snap_beneficiaries_year_2015': 'mean',\n",
    "    'bachelors_deg_year_2015': 'mean',\n",
    "    'bchecks_2015': 'mean',\n",
    "    'score_legis': 'mean',\n",
    "    'resident_pop_year_2015': 'mean',\n",
    "    'area': 'mean',\n",
    "    'per_dem' : 'mean'\n",
    "})\n",
    "\n",
    "states_data['pop_density'] = states_data['resident_pop_year_2015'] / states_data['area']\n",
    "states_data['bchecks_2015'] = states_data['bchecks_2015'] / states_data['resident_pop_year_2015']\n",
    "\n",
    "states_data.drop('area', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_data = merge_geo.loc[merge_geo['date'].str.startswith('2015')].groupby('fips').agg({\n",
    "    'n_killed': 'sum',\n",
    "    'n_injured': 'sum',\n",
    "    'incident_id': 'count',\n",
    "})\n",
    "\"\"\"\n",
    "incident_data['n_killed'] = incident_data['n_killed']/states_data['resident_pop_year_2015']\n",
    "incident_data['n_injured'] = incident_data['n_injured']/states_data['resident_pop_year_2015']\n",
    "incident_data['incident_id'] = incident_data['incident_id']/states_data['resident_pop_year_2015']\n",
    "\"\"\"\n",
    "incident_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_data = pd.merge(left=states_data, right=incident_data, on='fips')\n",
    "heat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g1 = sns.heatmap(heat_data.corr(), cmap='seismic', annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "heat_data = heat_data.dropna()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(heat_data[['med_h_income_year_2015', 'unemp_rate_year_2015',\n",
    "       'snap_beneficiaries_year_2015', 'bachelors_deg_year_2015',\n",
    "       'bchecks_2015', 'score_legis',\n",
    "       'pop_density']])\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(heat_data['n_killed'], X).fit(cov_type='HC3')\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def extract_features_selected(lasso: Pipeline, preprocessing_step_name: str = 'preprocess') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts selected features based on the coefficients obtained from Lasso regression.\n",
    "\n",
    "    Parameters:\n",
    "    - lasso (Pipeline): The scikit-learn pipeline containing a trained Lasso regression model.\n",
    "    - preprocessing_step_name (str): The name of the preprocessing step in the pipeline. Default is 'preprocess'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: A Pandas Series containing selected features with non-zero coefficients.\n",
    "    \"\"\"\n",
    "    # Check if lasso object is provided\n",
    "    if not isinstance(lasso, Pipeline):\n",
    "        raise ValueError(\"The provided lasso object is not a scikit-learn pipeline.\")\n",
    "\n",
    "    # Extract the final transformer from the pipeline\n",
    "    lasso_model = lasso[-1]\n",
    "\n",
    "    # Check if lasso_model is a Lasso regression model\n",
    "    if not isinstance(lasso_model, Lasso):\n",
    "        raise ValueError(\"The final step of the pipeline is not a Lasso regression model.\")\n",
    "\n",
    "    # Check if lasso model has 'coef_' attribute\n",
    "    if not hasattr(lasso_model, 'coef_'):\n",
    "        raise ValueError(\"The provided Lasso regression model does not have 'coef_' attribute. \"\n",
    "                         \"Make sure it is a trained Lasso regression model.\")\n",
    "\n",
    "    # Get feature names from the preprocessing step\n",
    "    features_preprocessing = lasso[preprocessing_step_name].get_feature_names_out()\n",
    "\n",
    "    # Extract selected features based on non-zero coefficients\n",
    "    features_selec = pd.Series(features_preprocessing[np.abs(lasso_model.coef_) > 0])\n",
    "\n",
    "    return features_selec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Echantillon d'entraînement et échantillon test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    heat_data.drop([\"n_killed\", 'n_injured'], axis = 1),\n",
    "    100*heat_data[['n_killed']], test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "\n",
    "numerical_features = X_train.select_dtypes(include='number').columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('number', numeric_pipeline, numerical_features),\n",
    "    ('category', categorical_pipeline, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(fit_intercept=True, alpha = 0.1)  \n",
    "\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "lasso_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "lasso1 = lasso_pipeline['model']\n",
    "features_selec = extract_features_selected(lasso_pipeline)\n",
    "#np.abs(lasso1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selec.str.replace(\"(number__|category__)\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import lasso_path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Utilisation de lasso_path\n",
    "preprocessed_features = pd.DataFrame(\n",
    "      numeric_pipeline.fit_transform(\n",
    "        X_train.drop(columns = categorical_features)\n",
    "      )\n",
    "  )\n",
    "my_alphas = np.array([0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0])\n",
    "\n",
    "alpha_for_path, coefs_lasso, _ = lasso_path(\n",
    "  preprocessed_features,\n",
    "  y_train,\n",
    "  alphas=my_alphas)\n",
    "#print(coefs_lasso)\n",
    "nb_non_zero = np.apply_along_axis(func1d=np.count_nonzero,arr=coefs_lasso,axis=0)\n",
    "nb_non_zero = pd.DataFrame(\n",
    "  nb_non_zero\n",
    ").sum(axis = 0)\n",
    "\n",
    "## graphique\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure()\n",
    "p = sns.lineplot(y=nb_non_zero, x=alpha_for_path)\n",
    "p.set(title = r\"Number variables and regularization parameter ($\\alpha$)\", xlabel=r'$\\alpha$', ylabel='Nb. de variables')\n",
    "p.figure.get_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
